<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-142902827-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-142902827-1');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon">

    <title>Tasnim</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900"
        rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i"
        rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <!-- <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet"> -->
    <link href="vendor/academicons/css/academicons.min.css" rel="stylesheet">
    <!-- <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="css/resume.css" rel="stylesheet">





</head>

<body id="page-top">
    <!-- ###*********************### -->
    <!-- ### Navigation side bar ### -->
    <!-- ###*********************### -->

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">
            <span class="d-none d-lg-block">
                <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/tasnim.png" alt="Tasnim">
            </span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="#about">About</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="#research">Research</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="#resources">Resources</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="resources/Resume.pdf" target="_blank">Résumé</a>
                </li>

                <!--
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#cv">CV</a>
          </li>
          -->
                <!--
                <li class="nav-item">
                    <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
                </li>
                -->
            </ul>
        </div>
    </nav>



    <div class="container-fluid p-0">

        <!-- ###*********************### -->
        <!-- ### Home Section        ### -->
        <!-- ###*********************### -->

        <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
            <div class="my-auto">

                <h1 class="mb-0">Tasnim
                    <span class="text-primary">Mohiuddin</span>
                </h1>


                <div class="subheading mb-5" aria-hidden="true" title="Researcher. Machine Learning Enthusiast.">.
                    <span class="typed"> </span>
                </div>



                <div class="row mb-5">

                    <!-- ###*********************### -->
                    <!-- ### About Subsection       ### -->
                    <!-- ###*********************### -->

                    <div class="col-lg-8 col-md-8 col-sm-12">
                        <p>I am currently working as a Researcher at Huawei Singapore Research Center. I obtained my
                            Ph.D. from <a href="https://www.ntu.edu.sg/Pages/home.aspx" target="_blank">Nanyang
                                Technological University (NTU)</a> in 2022, where I was a founding member of the <a
                                href="https://ntunlpsg.github.io" target="_blank">NTU-NLP Group</a> and was supervised
                            by Professor <a href="https://raihanjoty.github.io" target="_blank">Shafiq Joty</a>.
                            Notably, my doctoral dissertation was met with significant acclaim, leading to the conferral
                            of the distinguished <a
                                href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2023"
                                target="_blank">Outstanding Ph.D. Thesis Award</a>. I worked as a research intern at <a
                                href="https://ai.meta.com" target="_blank"> Meta AI</a> hosted by Professor <a
                                href="https://www.cs.jhu.edu/~phi/" target="_blank"> Philipp Koehn</a>.
                            <br>
                            My research interest falls broadly in the
                            application of Machine Learning, especially Natural Language Processing. I am also
                            interested in Multimodal Representation Learning. I invest considerable time and effort in
                            delving into the intricacies of large-scale models, aiming to unravel their nuanced
                            functionalities and underlying mechanisms.
                        </p>
                        <br>
                        <p>
                            <a href="resources/Resume.pdf" target="_blank"> [
                                <b>Résumé</b>]</a>
                        </p>
                    </div>


                    <!-- ###*************************### -->
                    <!-- ### Recent News  Subsection ### -->
                    <!-- ###*************************### -->

                    <div class="col-lg-4 col-md-4 col-sm-12">

                        <div class="subheading mb-3 text-primary">Recent News</div>
                        <ul class="fa-ul mb-0 term-list">


                            <li class="term-item">
                                <i class="fa-li fa fa-trophy"></i>
                                April 2023: Recieved prestigious <a
                                    href="https://www.ntu.edu.sg/scse/news-events/news/detail/scse-outstanding-phd-thesis-award-2023"
                                    target="_blank">Outstanding Ph.D. Thesis Award</a>. Super excited!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Oct 2022: Paper got accepted at
                                <a href="https://2022.emnlp.org" target="_blank">EMNLP 2022</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-rocket"></i>
                                June 2022: Joined Huawei Singapore Research Center!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-graduation-cap"></i>
                                May 2022: Finished oral defense of my Ph.D journey!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-facebook"></i>
                                May 2021: Started summer internship at <a href="https://ai.facebook.com"
                                    target="_blank">Facebook AI Research (FAIR)</a> under <a
                                    href="https://www.cs.jhu.edu/~phi/" target="_blank"> Philipp Koehn</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                May 2021: Two papers got accepted at
                                <a href="https://2021.aclweb.org" target="_blank">ACL 2021</a>!
                            </li>


                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Jan 2021: Paper got accepted at
                                <a href="https://2021.eacl.org" target="_blank">EACL 2021</a>!
                            </li>



                            <li class="term-item">
                                <i class="fa-li fa fa-bell"></i>
                                Nov 2020: Presented our paper at
                                <a href="http://virtual.2020.emnlp.org" target="_blank">virtual EMNLP 2020</a>!
                            </li>


                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Sept 2020: Our <a href="https://arxiv.org/abs/2004.13889" target="_blank">LNMap</a> got
                                accepted at
                                <a href="https://2020.emnlp.org" target="_blank">EMNLP 2020</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-bell"></i>
                                July 2020: Presented our journal paper at
                                <a href="https://acl2020.org/" target="_blank">ACL 2020</a> <a
                                    href="https://slideslive.com/38929481/unsupervised-word-translation-with-adversarial-autoencoder"
                                    target="_blank">virtually</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-university"></i>
                                Feb 2020: Defended my Qualifying Exam. Became a Ph.D. candidate. Yay!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Jan 2020: Our journal paper got accepted in
                                <a href="http://cljournal.org" target="_blank">Computational Linguistics</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Aug 2019: Our paper got accepted at
                                <a href="https://www.emnlp-ijcnlp2019.org" target="_blank">EMNLP-IJCNLP 2019</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-plane"></i>
                                June 2019: Travelled to Minneapolis to present our papers at NAACL!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Feb 2019: Two papers got accepted at
                                <a href="https://naacl2019.org" target="_blank">NAACL-HLT 2019</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                August 2018: Our journal accepted for publication in
                                <a href="http://cljournal.org" target="_blank">Computational Linguistics</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-plane"></i>
                                July 2018: Travelled to Melbourne to present our work at ACL!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-book"></i>
                                Apr 2018: Our paper on coherence model got accepted at
                                <a href="https://acl2018.org" target="_blank">
                                    ACL 2018</a>!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-university"></i>
                                Jan 2018: Started my Ph.D. journey at NTU!
                            </li>

                            <li class="term-item">
                                <i class="fa-li fa fa-address-card"></i>
                                Dec 2017: Moved to Singapore!
                            </li>

                        </ul>

                    </div>

                </div>


                <ul class="list-inline list-social-icons mb-0">
                    <li class="list-inline-item">
                        <a href="mailto:mohi0004@e.ntu.edu.sg" target="_blank" aria-label="Email">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i aria-hidden="true" title="Email" class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="https://twitter.com/mtaasnim" target="_blank" aria-label="Twitter">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i aria-hidden="true" title="Twitter" class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="https://www.linkedin.com/in/tasnimmohiuddin/" target="_blank" aria-label="Linkedin">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i aria-hidden="true" title="Linkedin"
                                    class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="https://github.com/taasnim" target="_blank" aria-label="Github">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i aria-hidden="true" title="Github" class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="https://scholar.google.com/citations?user=raW_wZYAAAAJ&hl=en" target="_blank"
                            aria-label="Google Scholar">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i aria-hidden="true" title="Google Scholar"
                                    class="ai ai-google-scholar fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                </ul>
            </div>
        </section>





        <!-- ###*************************### -->
        <!-- ### Publications Section    ### -->
        <!-- ###*************************### -->

        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
            <div class="my-auto">
                <h2 class="mb-5">Publications</h2>

                <!-- ###*************************### -->
                <!-- ### Conf Papers Subsection  ### -->
                <!-- ###*************************### -->


                <!--
                <div class="container-fluid">
                    <div class="subheading mb-3">Arxiv Preprint Papers </div>
                    <ol class="mb-0" type="1">


                        <li>
                            <p>
                                M Saiful Bari, <b>Tasnim Mohiuddin </b> (<i>Equal Contributions</i>), and Shafiq Joty,
                                <i>"UXLA: A Robust Data Augmentation Framework for Cross-Lingual NLP"</i>. <i>Arxiv
                                    preprint</i>, May 2020.

                                In Proceedings of the
                                <a href="https://www.emnlp-ijcnlp2019.org" target="_blank">
                                    Conference on Empirical Methods in Natural Language Processing and International
                                    Joint Conference on Natural Language Processing
                                </a>
                                <b>(EMNLP-IJCNLP 2019)</b>, Hong Kong, China.

                <br>
                <a href="https://arxiv.org/abs/2004.13240" target="_blank">[PDF]</a>

                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>


                                <a href="" target="_blank">[Code]</a>

                            </p>
                        </li>



                    </ol>
                </div>
                -->


                <div class="container-fluid">
                    <div class="subheading mb-3">Peer-reviewed Conference Papers </div>
                    <ol class="mb-0" type="1">



                        <li>
                            <p>
                                <b>Tasnim Mohiuddin </b>, Philipp Koehn, Vishrav Chaudhary, James Cross, Shruti Bhosale,
                                and Shafiq Joty, <i>"Data Selection Curriculum for Neural Machine Translation"</i>.
                                In Findings of
                                <a href="https://2022.emnlp.org" target="_blank">
                                    2022 Conference on Empirical Methods in Natural Language Processing
                                </a>
                                <b>(EMNLP 2022)</b>,
                                Abu Dhabi, UAE.

                                <br>
                                <a href="https://aclanthology.org/2022.findings-emnlp.113/" target="_blank">[PDF]</a>
                                <!--
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>

                                -->
                                <a href="" target="_blank">[Code]</a>
                            </p>
                        </li>


                        <li>
                            <p>
                                M Saiful Bari, <b>Tasnim Mohiuddin </b> (<i>Equal Contributions</i>), and Shafiq Joty,
                                <i>"UXLA: : A Robust Unsupervised Data Augmentation Framework for Cross-Lingual
                                    NLP"</i>.
                                In Proceedings of
                                <a href="https://2021.eacl.org" target="_blank">
                                    The Joint Conference of the 59th Annual Meeting of the Association for Computational
                                    Linguistics and the 11th International Joint Conference on Natural Language
                                    Processing
                                </a>
                                <b>(ACL-IJCNLP 2021)</b>.

                                <br>
                                <a href="https://aclanthology.org/2021.acl-long.154" target="_blank">[PDF]</a>
                                <!--
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>

                                -->
                                <a href="" target="_blank">[Code]</a>
                            </p>
                        </li>




                        <li>
                            <p>
                                <b>Tasnim Mohiuddin </b>, M Saiful Bari, and Shafiq Joty,
                                <i>"AugVic: Exploiting BiText Vicinity for Low-Resource NMT"</i>.
                                In Findings of
                                <a href="https://2021.eacl.org" target="_blank">
                                    The Joint Conference of the 59th Annual Meeting of the Association for Computational
                                    Linguistics and the 11th International Joint Conference on Natural Language
                                    Processing
                                </a>
                                <b>(ACL-IJCNLP 2021)</b>.

                                <br>
                                <a href="https://aclanthology.org/2021.findings-acl.267/" target="_blank">[PDF]</a>
                                <!--
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>

                                -->
                                <a href="" target="_blank">[Code]</a>
                            </p>
                        </li>



                        <li>
                            <p>
                                <b>Tasnim Mohiuddin </b>, Prathyusha Jwalapuram, Xiang Lin, and Shafiq Joty,
                                <i>"Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks"</i>.
                                In Proceedings of the
                                <a href="https://2021.eacl.org" target="_blank">
                                    16th conference of the European Chapter of the Association for Computational
                                    Linguistics
                                </a>
                                <b>(EACL 2021)</b>, Kyiv, Ukraine.

                                <br>
                                <a href="https://www.aclweb.org/anthology/2021.eacl-main.308/" target="_blank">[PDF]</a>
                                <!--
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>

                                -->
                                <a href="https://ntunlpsg.github.io/project/coherence/coh-eval/"
                                    target="_blank">[Resource]</a>
                            </p>
                        </li>


                        <li>
                            <p>
                                <b>Tasnim Mohiuddin </b>, M Saiful Bari, and Shafiq Joty,
                                <i>"LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through
                                    Non-Linear Mapping in Latent Space"</i>.
                                <!--
                                <i>Arxiv preprint</i>, April 2020.
                                -->
                                In Proceedings of the
                                <a href="https://2020.emnlp.org" target="_blank">
                                    Conference on Empirical Methods in Natural Language Processing
                                </a>
                                <b>(EMNLP 2020)</b>.

                                <br>
                                <a href="https://www.aclweb.org/anthology/2020.emnlp-main.215/"
                                    target="_blank">[PDF]</a>

                                <a href="https://slideslive.com/38938789" target="_blank">[Presentation]</a>
                                <a href="https://github.com/taasnim/lnmap" target="_blank">[Code]</a>
                            </p>
                        </li>



                        <li>
                            <p>
                                Han-Cheol Moon,
                                <b>Tasnim Mohiuddin </b> (<i>Equal Contributions</i>), Shafiq Joty, and Chi Xu,
                                <i>"A Unified Neural Coherence Model"</i>. In Proceedings of the
                                <a href="https://www.emnlp-ijcnlp2019.org" target="_blank">
                                    Conference on Empirical Methods in Natural Language Processing and International
                                    Joint Conference on Natural Language Processing
                                </a>
                                <b>(EMNLP-IJCNLP 2019)</b>, Hong Kong, China.
                                <br>
                                <a href="https://www.aclweb.org/anthology/D19-1231.pdf" target="_blank">[PDF]</a>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ef-ebiRdhVtNilVDnDLzflIBth2oxgVg-TQiUkkKFGoycg?e=xCY2KR"
                                    target="_blank">[Presentation]</a>
                                <!--   <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/ETsm3OTxlJhPhvKYKGEQf4IB-OxuBQmGCy4Ne5RiORgSwQ?e=UHwXFf"
                                        target="_blank">[Poster]</a>
                                    -->
                                <a href="https://github.com/taasnim/unified-coherence-model" target="_blank">[Code]</a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <b>Tasnim Mohiuddin </b> and Shafiq Joty,
                                <i>"Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle
                                    Consistency
                                    and Improved Training"</i>. In Proceedings of the
                                <a href="https://naacl2019.org" target="_blank">
                                    North American Chapter of the Association for Computational Linguistics: Human
                                    Language Technologies
                                </a>
                                <b>(NAACL-HLT 2019)</b>, Minneapolis, USA.
                                <br>
                                <a href="https://www.aclweb.org/anthology/N19-1386/" target="_blank">[PDF]</a>
                                <a href="https://entuedu-my.sharepoint.com/:p:/g/personal/mohi0004_e_ntu_edu_sg/EXtQZl16XxxPm2wLTAYo9M0BaPqHmgyWB3Du79eaQw8Y_A?e=nz5E0d"
                                    target="_blank">[Presentation]</a>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/ETsm3OTxlJhPhvKYKGEQf4IB-OxuBQmGCy4Ne5RiORgSwQ?e=UHwXFf"
                                    target="_blank">[Poster]</a>
                                <a href="https://github.com/taasnim/unsup-word-translation/" target="_blank">[Code]</a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <b> Tasnim Mohiuddin </b>, Thanh-Tung Nguyen, and Shafiq Joty,
                                <i>"Adaptation of Hierarchical Structured Models for Speech Act Recognition in
                                    Asynchronous
                                    Conversation"
                                </i>. In Proceedings of the
                                <a href="https://naacl2019.org" target="_blank">
                                    North American Chapter of the Association for Computational Linguistics: Human
                                    Language Technologies
                                </a>
                                <b>(NAACL-HLT 2019)</b>, Minneapolis, USA.
                                <br />
                                <a href="https://aclanthology.org/N19-1134/" target="_blank">[PDF]</a>
                                <a href="https://entuedu-my.sharepoint.com/:p:/g/personal/mohi0004_e_ntu_edu_sg/ETxMhec85sFMiPMywRbpB8YBIudivHIiVsCfUNNVwK7R_g?e=hdvcrc"
                                    target="_blank">[Presentation]</a>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/ER83I9JoJ_NEtRFWFMPbsWwBhUy7zh6Z5OHPVQ5WpKE5eQ?e=ffw36J"
                                    target="_blank">[Poster]</a>
                                <a href="https://github.com/taasnim/speech-act-hierarchical/" target="_blank">[Code]</a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <b> Tasnim Mohiuddin </b>, Shafiq Joty, and Dat Nguyen,
                                <i>"Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid
                                    Approach"</i>. In
                                Proceedings of the
                                <a href="https://acl2018.org" target="_blank"> 56th Annual Meeting of the Association
                                    for Computational Linguistics </a>
                                <b>(ACL 2018)</b>, Melbourne, Australia.
                                <br />
                                <a href="https://www.aclweb.org/anthology/P18-1052" target="_blank">[PDF]</a>
                                <!--
                            <a href="https://entuedu-my.sharepoint.com/:p:/g/personal/mohi0004_e_ntu_edu_sg/ETxMhec85sFMiPMywRbpB8YBIudivHIiVsCfUNNVwK7R_g?e=hdvcrc" target="_blank">[presentation]</a>
                            -->
                                <a href="https://www.aclweb.org/anthology/attachments/P18-1052.Poster.pdf"
                                    target="_blank">[Poster]</a>
                                <a href="https://github.com/taasnim/conv-coherence/" target="_blank">[Code]</a>
                            </p>
                        </li>



                    </ol>
                </div>


                <!-- ###****************************### -->
                <!-- ### Journal Papers Subsection  ### -->
                <!-- ###****************************### -->

                <div class="container-fluid">
                    <div class="subheading mb-3">Peer-reviewed Journal Papers </div>

                    <ol class="mb-0" type="1">

                        <li>
                            <p>
                                <b>Tasnim Mohiuddin</b> and
                                Shafiq Joty,
                                <i>"Unsupervised Word Translation with Adversarial Autoencoder"</i>.
                                <strong>
                                    <a href="http://cljournal.org" target="_blank"> Computational Linguistics </a>
                                </strong> (Special Issue of Computational Linguistics on Multilingual and Interlingual
                                Semantic Representations for Natural Language Processing) : pages XXX - XXX (2020),
                                <a href="" target="_blank"> MIT press </a> (June 2020).
                                <br />
                                <a href="https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00374"
                                    target="_blank">[PDF]</a>
                                <a href="https://slideslive.com/38929481/unsupervised-word-translation-with-adversarial-autoencoder"
                                    target="_blank">[Presentation]</a>
                            </p>
                        </li>


                        <li>
                            <p>
                                Shafiq Joty and
                                <b>Tasnim Mohiuddin</b>,
                                <i>"Modeling Speech Acts in Asynchronous Conversations: A Neural-CRF Approach"</i>.
                                <strong>
                                    <a href="http://cljournal.org" target="_blank"> Computational Linguistics </a>
                                </strong> (Special Issue on Language in Social Media)
                                <b> 44:4 </b>, pages 859 - 894 (2018),
                                <a href="https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00339"
                                    target="_blank"> MIT press </a> (2018).
                                <br />
                                <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00339"
                                    target="_blank">[PDF]</a>
                                <a href="https://github.com/taasnim/speech-act/" target="_blank">[Code]</a>
                            </p>
                        </li>


                    </ol>
                </div>


            </div>
        </section>






        <!-- ###*****************************### -->
        <!-- ### Resarch Projects Section    ### -->
        <!-- ###*****************************### -->


        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
            <div class="my-auto">
                <div class="container-fluid">

                    <h2 class="mb-5">Research</h2>




                    <!-- ###*************************### -->
                    <!-- ###  Research Project#0     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Curriculum Learning for Neural Machine Translation</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        Neural Machine Translation (NMT) models are typically trained on heterogeneous
                                        data that are concatenated and randomly shuffled. However, not all of the
                                        training data are equally useful to the model. Curriculum training aims to
                                        present the data to the NMT models in a meaningful order.
                                        <br>
                                        In this project, we introduce a two-stage training framework for NMT where we
                                        fine-tune a base NMT model on subsets of data, selected by both deterministic
                                        scoring using pre-trained methods and online scoring that considers prediction
                                        scores of the emerging NMT model.
                                        <br>
                                        Through comprehensive experiments on six language pairs comprising low- and
                                        high-resource languages, we have shown that our curriculum strategies
                                        consistently demonstrate better quality and faster convergence.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://aclanthology.org/2022.findings-emnlp.113/" target="_blank">
                                            EMNLP-2022 </a>
                                    </p>

                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/ccl/ccl.png" alt="ccl-nmt">
                                </div>

                            </div>

                        </div>

                    </div>






                    <!-- ###*************************### -->
                    <!-- ###  Research Project#-1     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Image Editing in the WILD</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        The ability to edit images in a realistic and visually appealing manner is a
                                        fundamental requirement in various computer vision applications. In this paper,
                                        we present <b>ImEW</b>, a unified framework designed for solving image editing
                                        tasks.
                                        <br>
                                        <b>ImEW</b> utilizes off-the-shelf foundation models to address four essential
                                        editing tasks: object removal, object translation, object replacement, and
                                        generative fill beyond the image frame. These tasks are accomplished by
                                        leveraging the capabilities of state-of-the-art foundation models, namely the
                                        Segment Anything Model, Grounding DINO, LaMa, and Stable Diffusion. These models
                                        have undergone extensive training on large-scale datasets and have exhibited
                                        exceptional performance in understanding image context, object manipulation, and
                                        texture synthesis.
                                        <br>
                                        Through extensive experimentation, we demonstrate the effectiveness and
                                        versatility of <b>ImEW</b> in accomplishing image editing tasks across a wide
                                        range of real-world scenarios. The proposed framework opens up new possibilities
                                        for realistic and visually appealing image editing and enables diverse
                                        applications requiring sophisticated image modifications. Additionally, we
                                        discuss the limitations and outline potential directions for future research in
                                        the field of image editing using off-the-shelf foundation models, enabling
                                        continued advancements in this domain.
                                        <br>
                                        <br>
                                        <!--           
                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://aclanthology.org/2022.findings-emnlp.113/" target="_blank">
                                            EMNLP-2022 </a>
                                    </p>
                                    -->
                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/imew/imew.png" alt="ccl-nmt">
                                </div>

                            </div>

                        </div>

                    </div>





                    <!-- ###*************************### -->
                    <!-- ###  Research Project#1     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Exploiting BiText Vicinity for Low-Resource NMT</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        The success of Neural Machine Translation (NMT) largely depends on the
                                        availability of large bitext training corpora. Due to the lack of such large
                                        corpora in low-resource language pairs, NMT systems often exhibit poor
                                        performance. Extra relevant monolingual data often helps, but acquiring it could
                                        be quite expensive, especially for low-resource languages. Moreover, domain
                                        mismatch between bitext (train/test) and monolingual data might degrade the
                                        performance. To alleviate such issues, we propose <b>AugVic</b>, a novel data
                                        augmentation framework for low-resource NMT which exploits the vicinal samples
                                        of the given bitext without using any extra monolingual data explicitly.
                                        <br>
                                        It can diversify the in-domain bitext data with finer-level control. Through
                                        extensive experiments on four low-resource language pairs comprising data from
                                        different domains, we have shown that our method is comparable to the
                                        traditional back-translation that uses extra in-domain monolingual data. When we
                                        combine the synthetic parallel data generated from <b>AugVic</b> with the ones
                                        from the extra monolingual data, we achieve further improvements. We show that
                                        <b>AugVic</b> helps to attenuate the discrepancies between relevant and
                                        distant-domain monolingual data in traditional back-translation.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://aclanthology.org/2021.findings-acl.267" target="_blank">
                                            ACL-2021 </a>
                                    </p>

                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/data-aug/augvic.png" alt="augvic">
                                </div>

                            </div>

                        </div>

                    </div>



                    <!-- ###*************************### -->
                    <!-- ###  Research Project#2     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Data Augmentation for Cross-Lingual NLP</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        Transfer learning has yielded state-of-the-art (SoTA) results in many supervised
                                        natural language processing tasks. However, annotated data for every target task
                                        in every target language is rare, especially for low-resource languages. We
                                        target to solve <b>cross-lingual adaptation problems</b> from a source language
                                        distribution to an unknown target language distribution, assuming no training
                                        labels are available for the target language task.
                                        <br>
                                        In this project, we propose <b>UXLA</b>, a generic data augmentation
                                        framework for
                                        self-supervised learning in zero-resource transfer learning scenarios. At its
                                        core, UXLA performs simultaneous <i>self-training with data augmentation and
                                            unsupervised sample selection</i>. We augment data from the unlabeled
                                        training examples in the target language as well as from the virtual input
                                        samples (eg sentences) generated from the vicinity distribution of the source
                                        and target language sentences. With the augmented data, UXLA performs
                                        simultaneous self-learning with an effective distillation strategy to learn a
                                        strongly adapted cross-lingual model from noisy (pseudo) labels for the target
                                        language task. We propose novel ways to generate virtual input samples using
                                        XLMR - a multilingual masked language model, and get reliable task labels by
                                        simultaneous multilingual co-training.

                                        <br>
                                        To show our proposed methods' effectiveness, we conduct
                                        extensive experiments on zero-resource cross-lingual transfer tasks for Named
                                        Entity Recognition (<b>XNER</b>) and Natural Language Inference (<b>XNLI</b>).
                                        UXLA achieves SoTA results in both tasks, outperforming the baselines by a
                                        good margin. With an in-depth model dissection, we demonstrate the cumulative
                                        contributions of different components to UXLA's success.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://aclanthology.org/2021.acl-long.154/" target="_blank"> ACL-2021
                                        </a>
                                    </p>

                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/data-aug/uxla2.png" alt="uxla">
                                </div>

                            </div>

                        </div>

                    </div>


                    <!-- ###*************************### -->
                    <!-- ###  Research Project#3     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Bilingual Lexicon Induction with Limited Supervision</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        Most of the successful and predominant methods for Bilingual Lexicon Induction
                                        (BLI) are mapping-based, where a linear mapping function is learned with the
                                        assumption that the word embedding spaces of different languages exhibit similar
                                        geometric structures i.e. approximately <i>isomorphic})</i>. However, several
                                        recent studies have criticized this simplified assumption showing that it does
                                        not hold in general even for closely related languages. In this work, we propose
                                        a novel semi-supervised method to <i>learn cross-lingual word embeddings for
                                            BLI</i>. Our model is independent of the isomorphic assumption and uses
                                        non-linear mapping in the latent space of two independently pre-trained}
                                        autoencoders. Through extensive experiments on fifteen (15) different language
                                        pairs (in both directions) comprising resource-rich and low-resource languages
                                        from two different datasets, we demonstrate that our method outperforms existing
                                        models by a good margin. Ablation studies show the importance of different model
                                        components and the necessity of non-linear mapping.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://www.aclweb.org/anthology/D19-1231/" target="_blank"> EMNLP-2020
                                        </a>
                                    </p>

                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/lnmap/model.png" alt="LNMap">
                                </div>

                            </div>

                        </div>

                    </div>


                    <!-- ###*************************### -->
                    <!-- ###  Research Project#3     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Unsupervised word translation</h3>
                            <!--
	              <div class="subheading mb-3">Supervisors: Darren Gergle, Anne Marie Piper</div>
                -->
                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        Suppose we are given
                                        <i>monolingual word embeddings</i> for source and target languages. We do not
                                        have any
                                        initial dictionary or external cross-lingual signal. Our
                                        <b>goal</b> is to learn word translation (a.k.a. bilingual lexicon induction)
                                        i.e. for
                                        a given source word, we want its translation in the target domain.
                                        <br> Cross-lingual word embeddings learned from monolingual embeddings have a
                                        crucial
                                        role in many downstream tasks, ranging from machine translation to transfer
                                        learning.
                                        <i>Adversarial training</i> has shown impressive success in learning
                                        cross-lingual embeddings
                                        and the associated word translation task without any parallel data by mapping
                                        monolingual
                                        embeddings to a shared space. In this project, we investigate
                                        <b>adversarial autoencoder</b> for unsupervised word translation and propose
                                        <i>two novel extensions</i> to it that yield more stable training and improved
                                        results.
                                        Our method includes
                                        <b>regularization terms</b> to enforce
                                        <i>cycle consistency</i> and
                                        <i>input reconstruction</i>, and puts the target encoders as an adversary
                                        against the
                                        corresponding discriminator.
                                        <br> We use two types of
                                        <b>refinement procedures</b> sequentially after obtaining the trained encoders
                                        and mappings
                                        from the adversarial training, namely,
                                        <i>refinement with Procrustes solution</i> and
                                        <i>refinement with symmetric re-weighting</i>.
                                        <br> Extensive experimentations with European, non-European and low-resource
                                        languages
                                        show that our method achieves better performance than existing adversarial and
                                        non-adversarial
                                        approaches and is also competitive with the supervised system. Along with
                                        performing
                                        comprehensive ablation studies to understand the contribution of different
                                        components
                                        of our adversarial model, we also conduct a thorough analysis of the refinement
                                        procedures
                                        to understand their effects.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://www.aclweb.org/anthology/N19-1386/" target="_blank"> NAACL-HLT
                                            2019 </a>,
                                        <a href="https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00374"
                                            target="_blank"> CL Journal 2020</a>
                                    </p>

                                    </p>
                                </div>


                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/unsup-word-translation/model-2.png"
                                        alt="unsupervised word translation models">
                                </div>

                            </div>

                        </div>

                    </div>






                    <!-- ###*************************### -->
                    <!-- ###  Research Project#3    ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Neural Coherence Model</h3>

                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        Sentences in a text or a conversation do not occur independently; rather they
                                        are connected to form a coherent discourse
                                        that is easy to comprehend.
                                        <b>Coherence models</b> are computational models that can distinguish a coherent
                                        discourse
                                        from incoherent ones. It has ranges of applications in text generation,
                                        summarization,
                                        and coherence scoring.
                                        <br> In this project, we conduct our research in
                                        <i>two steps</i>.
                                        <br>
                                        <b>First</b>, we propose improvements to the recently proposed
                                        <i>neural entity grid</i> model by
                                        <b>lexicalizing</b> its entity transitions. We propose methods based on word
                                        embeddings
                                        to achieve better generalization with the lexicalized model.
                                        <br>

                                        <b>Second</b>, we extend the model to asynchronous conversations by
                                        incorporating the
                                        underlying conversational structure in the entity grid representation and
                                        feature
                                        computation. For this, we propose a novel grid representation for asynchronous
                                        conversations
                                        and adapt the convolution layer of the neural model accordingly.
                                        <br> Our model achieves state of the art results on standard coherence
                                        assessment tasks
                                        in monologue and conversations outperforming existing models. We also
                                        demonstrate
                                        its effectiveness in reconstructing thread structures.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://www.aclweb.org/anthology/P18-1052" target="_blank"> ACL
                                            2018 </a>
                                    </p>
                                    </p>
                                </div>

                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/conv-coherence/models.png" alt="model-acl-18">
                                </div>

                            </div>

                        </div>

                    </div>


                    <!-- ###*************************### -->
                    <!-- ###  Research Project#4     ### -->
                    <!-- ###*************************### -->

                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary">Unified Coherence Model</h3>

                            <div class="row mb-5">
                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        In this project, we work on the limitations of existing models which
                                        underperform on tasks that require the model to be sensitive
                                        to local contexts such as candidate ranking in conversational dialogue and in
                                        machine
                                        translation.
                                        <br> We propose a
                                        <i>unified coherence model</i> that incorporates
                                        <b>sentence grammar</b>,
                                        <b>inter-sentence coherence relations</b>, and
                                        <b>global coherence patterns</b> in a single framework. We use an
                                        <i>LSTM sentence encoder with explicit language model loss</i> to capture the
                                        syntax.
                                        Inter-sentence discourse relations are modeled with a
                                        <i>bilinear layer</i>, and a
                                        <i>lightweight convolution-pooling</i> is used to capture the attention and
                                        topic structures
                                        (global coherence patterns ).
                                        <br> We evaluate our models on both local and global discrimination tasks on the
                                        benchmark
                                        dataset. Our results show that our approach outperforms existing methods by a
                                        wide
                                        margin in both tasks.
                                        <br>
                                        <br>

                                    <p>
                                        <b>Publication: </b>
                                        <a href="https://www.aclweb.org/anthology/D19-1231/" target="_blank">
                                            EMNLP-IJCNLP
                                            2019 </a>
                                    </p>
                                    </p>
                                </div>

                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/unified-coherence/model-emnlp-2019.png"
                                        alt="model-emnlp-2019">
                                </div>

                            </div>

                        </div>

                    </div>



                    <!-- ###*************************### -->
                    <!-- ###  Research Project#5    ### -->
                    <!-- ###*************************### -->


                    <div class="resume-item d-flex flex-column flex-md-row mb-4">
                        <div class="resume-content mr-auto">
                            <h3 class="mb-0 text-primary"> Speech Act Recognition in Asynchronous Conversation</h3>
                            <div class="row mb-5">

                                <div class="col-lg-8 col-md-8 col-sm-12">
                                    <p align="justify">
                                        With the advent of Internet technologies, communication media like emails and
                                        discussion forums have become common-place
                                        for discussing work, issues, events, and experiences. Participants in these
                                        media
                                        interact with each other
                                        <i>asynchronously</i> by writing at different times. Participants in an
                                        asynchronous
                                        conversation interact with each other in complex ways, performing certain
                                        communicative
                                        acts like asking questions, requesting information or suggesting something.
                                        These
                                        are called
                                        <b>speech acts</b>.
                                        <br> Unlike synchronous conversations (e.g. meeting, phone), modeling
                                        conversational
                                        dependencies between sentences in an asynchronous conversation are challenging.
                                        The
                                        conversational flow often lacks sequential dependencies in its
                                        temporal/chronological
                                        order.
                                        <b>For example</b>, if we arrange the sentences as they arrive in the
                                        conversation,
                                        it becomes hard to capture any dependency between the acts types because the two
                                        components of the adjacency pairs can be far apart in the sequence. This leaves
                                        us
                                        with
                                        <b>one open research question</b>:
                                        <i>how do we model the dependencies between sentences in a single comment and
                                            between
                                            sentences across different comments?</i> Another major problem is
                                        <i>insufficient training data</i> in the asynchronous domains.
                                        <br> In this project, we propose methods to effectively leverage abundant
                                        unlabeled conversational
                                        data and the available labeled data from synchronous domains. We carry out our
                                        research
                                        in
                                        <i>three main steps. </i>
                                        <br>

                                        <b>First</b>, we introduce an end-to-end neural architecture based on a
                                        hierarchical
                                        LSTM encoder with a Softmax or conditional random fields (CRF) output layer, and
                                        show that our method outperforms existing methods when trained on in-domain data
                                        only.
                                        <br>

                                        <b>Second</b>, we improve our initial SAR models by semi-supervised learning in
                                        the
                                        form of pretrained word embeddings learned from a large unlabeled conversational
                                        corpus.
                                        <br>

                                        <b>Finally</b>, we adapt our hierarchical LSTM encoder using domain adversarial
                                        training
                                        to leverage the labeled data from synchronous domains by explicitly modeling the
                                        shift in the two domains.
                                        <br> We evaluate our models on three different asynchronous datasets containing
                                        forum
                                        and email conversations, and on the MRDA meeting corpus. Our main findings are:
                                        <br>

                                    <ol type="i">
                                        <li>hierarchical LSTMs outperform existing methods when trained on
                                            in-domain
                                            data for both synchronous and asynchronous domains, setting a new
                                            state-of-the-art</li>
                                        <li>conversational word embeddings yield significant improvements over
                                            off-the-shelf
                                            ones
                                        </li>
                                        <li>domain adversarial training improves the results by inducing
                                            domain-invariant
                                            features.
                                        </li>
                                    </ol>

                                    <p>
                                        <b>Publications: </b>
                                        <a href="https://www.mitpressjournals.org/doi/full/10.1162/coli_a_00339"
                                            target="_blank"> CL Journal 2018</a>,
                                        <a href="https://www.aclweb.org/anthology/N19-1134" target="_blank"> NAACL-HLT
                                            2019 </a>
                                    </p>
                                    </p>
                                </div>

                                <div class="col-lg-4 col-md-4 col-sm-12">
                                    <img class="img-fluid" src="img/speech-act/speech-act-model.png"
                                        alt="speech-act-model">
                                </div>

                            </div>

                        </div>

                    </div>


                    <!-- ###*************************### -->
                    <!-- ###  Research Project#5     ### -->
                    <!-- ###*************************### -->

                </div>
            </div>
        </section>



        <!-- ###*************************### -->
        <!-- ### Resources Section    ### -->
        <!-- ###*************************### -->

        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="resources">
            <div class="my-auto">
                <h2 class="mb-5">Resources</h2>
                My presentations on different topics and papers:
                <br>
                <br>
                <!-- ###*************************### -->
                <!-- ### Topics Subsection  ### -->
                <!-- ###*************************### -->

                <div class="container-fluid">
                    <div class="subheading mb-3">Topics </div>

                    <ul type="square">
                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EY1SUxetzZVJh_eHeVHGS6sBqnXJopLFy7t5xv2pS2KHIw?e=qPca3n"
                                    target="_blank">
                                    <b>Linear Regression</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EX0y1B1DOfZNgLYZqfR9qfABiJ04wgvH0Y7S9Oq1ghc4Qw?e=Etgwby"
                                    target="_blank">
                                    <b>Logistic Regression</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EWbC4RLvV3lPjeL-PFHKZqYBfFRHikMm0pcIl4z7GsCT9A?e=sfzETr"
                                    target="_blank">
                                    <b>Generative Models for Discrete Data</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EQ2kqIHkxe5FmqaAUHuAbVMBL4zgw9kvehrFmRYhpOT-iA?e=rDsOO8"
                                    target="_blank">
                                    <b>Naive Bayes Classifiers and Gaussian Models</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EZ8qcId1gvNHu_7DG5eZqzcBWONhkkvQZrhYgwNxi-5e-Q?e=ySGtrd"
                                    target="_blank">
                                    <b>Recurrent Neural Networks</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/EX2IsgI-r-RAjaVyZtwrpSgBdVAL1oGtEbSP8nIgjZ4ISQ?e=EDYufy"
                                    target="_blank">
                                    <b>Cross-lingual Word Embeddings</b>
                                </a>
                            </p>
                        </li>



                    </ul>
                </div>



                <!-- ###*************************### -->
                <!-- ### Papers Subsection  ### -->
                <!-- ###*************************### -->

                <div class="container-fluid">
                    <div class="subheading mb-3">Papers </div>

                    <ul type="square">
                        <li>
                            <p>
                                <a href="https://drive.google.com/file/d/1W2BaUNc5IqaDypNiXcb0MweOtCetUqZm/view"
                                    target="_blank">
                                    <b>Sequence to Sequence Learning with Neural Networks</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://drive.google.com/file/d/1niMR8LX77DnP_iPzjNRauOdz1wjd_eXp/view"
                                    target="_blank">
                                    <b>Neural Machine Translation by Jointly Learning to Align and Translate</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://drive.google.com/file/d/1tcUFCgyswf6RPvV9ZWHcOWLOF1xe4vGT/view"
                                    target="_blank">
                                    <b>Revisiting Semi-Supervised Learning with Graph Embeddings</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://drive.google.com/file/d/1vV3v69el71Sq53IlH3OtWcpsn6o0kCGD/view"
                                    target="_blank">
                                    <b>Improving Language Understanding by Generative Pre-Training</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://drive.google.com/file/d/1Fb7ki6zxtnXT2Fd9Xfqt2uwSV1AiL9Du/view"
                                    target="_blank">
                                    <b>Cross-lingual Language Model Pretraining</b>
                                </a>
                            </p>
                        </li>

                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Efvm07uqtSRHt088xwAKruUBFV79X3Buw5agZkWF4bxAIA?e=MRGXnJ"
                                    target="_blank">
                                    <b>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language
                                        Generation, Translation, and Comprehension </b>
                                </a>
                            </p>
                        </li>





                        <li>
                            <p>
                                <a href="https://entuedu-my.sharepoint.com/:b:/g/personal/mohi0004_e_ntu_edu_sg/Ee-9h7ptIopKu-OYd1jYI5ABq-QlUJ2NGG9GmzbJTgKpXg?e=Xqm9oU"
                                    target="_blank">
                                    <b>Self-training Improves Pre-training for Natural Language Understanding </b>
                                </a>
                            </p>
                        </li>







                    </ul>
                </div>

            </div>
        </section>


        <!--

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="cv">
        <div class="my-auto">
          <h2 class="mb-5">CV</h2>

          <div class="subheading mb-3">
          	<a href="https://app.box.com/s/rnoqxbdx0bvn6nxbr5djyizcg97nusce" target="_blank">CV</a>
          </div>
          <div class="subheading mb-3">
          	<a href="https://app.box.com/s/rrs2dwe1sgxwsboxdohg0tqtlxg4cplh" target="_blank">Resume</a>
          </div>
        </div>
      </section>

    -->


        <!-- ###*************************### -->
        <!-- ###  Contact Section        ### -->
        <!-- ###*************************### -->
        <!--
        <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="contact">
            <div class="my-auto">
                <h2 class="mb-5">Contact</h2>
                <p>
                    <b> Multimedia and Interactive Computing Lab (MICL) </b>
                    <br> Block N4 #B1C-17
                    <br> School of Computer Science and Engineering
                    <br> Nanyang Technological University
                    <br> Singapore 639798.
                </p>
                
            </div>
        </section>
    -->
    </div>



    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.js"></script>
    <script src="vendor/typed/typed.js"></script>
    <!--   <script src="vendor/isotope/isotope.pkgd.js"></script>
    <script src="vendor/magnific-popup/magnific-popup.js"></script> -->
</body>

</html>